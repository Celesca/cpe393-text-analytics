{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8ixLmMFuOWT"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR8OAG-J6wz1"
      },
      "source": [
        "# One-hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qDKM2tAC6wz2"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"It was the best of times\",\n",
        "    \"it was the worst of times\",\n",
        "    \"it was the age of wisdom\",\n",
        "    \"it was the age of foolishness\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDkcykj4pzkx"
      },
      "source": [
        "Tokenize all sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rzC65J026wz3",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['It', 'was', 'the', 'best', 'of', 'times'],\n",
              " ['it', 'was', 'the', 'worst', 'of', 'times'],\n",
              " ['it', 'was', 'the', 'age', 'of', 'wisdom'],\n",
              " ['it', 'was', 'the', 'age', 'of', 'foolishness']]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_sentences = [[t for t in sentence.split()] for sentence in sentences]\n",
        "tokenized_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J0vfFWXttBH"
      },
      "source": [
        "Create a vocabulary containing unique words from all sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aJut9Urd6wz3",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'It',\n",
              " 'age',\n",
              " 'best',\n",
              " 'foolishness',\n",
              " 'it',\n",
              " 'of',\n",
              " 'the',\n",
              " 'times',\n",
              " 'was',\n",
              " 'wisdom',\n",
              " 'worst'}"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocabulary = set([w for s in tokenized_sentences for w in s])\n",
        "vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lShvWmhuuxyI"
      },
      "source": [
        "Encode each token in a sentence by assigning 1 if the token is present in a sentence, else assigning 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RxE4EUiZ6wz4"
      },
      "outputs": [],
      "source": [
        "def onehot_encoder(tokenized_sentence):\n",
        "    return [1 if w in tokenized_sentence else 0 for w in vocabulary]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ynOkTS_V6wz4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0]: It was the best of times\n",
            "[1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0]: it was the worst of times\n",
            "[1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1]: it was the age of wisdom\n",
            "[1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1]: it was the age of foolishness\n"
          ]
        }
      ],
      "source": [
        "onehot = [onehot_encoder(tokenized_sentence)\n",
        "          for tokenized_sentence in tokenized_sentences]\n",
        "for (sentence, oh) in zip(sentences, onehot):\n",
        "    print(\"%s: %s\" % (oh, sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5k8nTbE6wz4"
      },
      "source": [
        "### Out-of-vocabulary documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5GqMQ5M6wz4"
      },
      "source": [
        "#### All tokens are known"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bQdxKIP06wz4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onehot_encoder(\"the age of wisdom is the best of times\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i87FRh4n6wz5"
      },
      "source": [
        "#### Some tokens are not known\n",
        "\n",
        "This could be a problem..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X3IA3Zdk6wz5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "onehot_encoder(\"John likes to watch movies\".split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPZWwXwC6wz5"
      },
      "source": [
        "# Bag-of-Words Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iu-95hIS6wz5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hcvXDzsm6wz5"
      },
      "outputs": [],
      "source": [
        "more_sentences = sentences + \\\n",
        "                 [\"John likes to watch movies. Mary likes movies too.\",\n",
        "                  \"Mary also like to watch football games.\"]\n",
        "more_sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kej0rzIo6wz5"
      },
      "outputs": [],
      "source": [
        "cv.fit(more_sentences)\n",
        "print(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhBHLdNa6wz5"
      },
      "outputs": [],
      "source": [
        "dt = cv.transform(more_sentences)\n",
        "dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98DLcnxN6wz5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qi1b6HettCq"
      },
      "source": [
        "Another example:\n",
        "\n",
        "“Oh, honey, I would walk through fire for you”\n",
        "\n",
        "“Just let me adore you”\n",
        "\n",
        "“Like it is the only thing I will ever do”\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tP4EGCvUt3iq"
      },
      "outputs": [],
      "source": [
        "another_example = [\"Oh, honey, I would walk through fire for you\",\n",
        "                   \"Just let me adore you\",\n",
        "                   \"Like it is the only thing I will ever do\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBvv1nKXyLuN"
      },
      "source": [
        "Creating a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRpktPXVyN4x"
      },
      "outputs": [],
      "source": [
        "tokenized2 = [[t for t in sentence.split()] for sentence in another_example]\n",
        "\n",
        "vocabulary2 = set([w.lower() for s in tokenized2 for w in s])\n",
        "vocabulary2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4O9SaVvgxLeN"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(stop_words=[], vocabulary=vocabulary2).fit(another_example)\n",
        "print(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa0ED3B8xWqa"
      },
      "outputs": [],
      "source": [
        "len(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EzWVSlU4HxR"
      },
      "outputs": [],
      "source": [
        "dt_full = cv.transform(another_example)\n",
        "pd.DataFrame(dt_full.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSPqu_ZLwyzV"
      },
      "source": [
        "Let's also apply stopwords removal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fQ92TS3uzV-"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(stop_words='english').fit(another_example)\n",
        "print(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOIjMI7Gu5cF"
      },
      "outputs": [],
      "source": [
        "len(cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc1Pg1Tfvumf"
      },
      "outputs": [],
      "source": [
        "dt = cv.transform(another_example)\n",
        "pd.DataFrame(dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAx3Atv26wz6"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94f4k8Lo6wz6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf = TfidfTransformer()\n",
        "tfidf_dt = tfidf.fit_transform(dt)\n",
        "pd.DataFrame(tfidf_dt.toarray(), columns=cv.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ibejaI6wz5"
      },
      "source": [
        "# Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zeqCQvb6wz5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "pd.DataFrame(cosine_similarity(dt))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11.1 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
